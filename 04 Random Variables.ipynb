{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41fe488",
   "metadata": {},
   "source": [
    "<img src='./fig/vertical_COMILLAS_COLOR.jpg' style= 'width:70mm'>\n",
    "\n",
    "<h1 style='font-family: Optima;color:#ecac00'>\n",
    "Máster en Big Data. Tecnología y Analítica Avanzada (MBD).\n",
    "<a class=\"tocSkip\">\n",
    "</h1>\n",
    "\n",
    "<h1 style='font-family: Optima;color:#ecac00'>\n",
    "Fundamentos Matemáticos del Análisis de Datos (FMAD). 2022-2023.\n",
    "<a class=\"tocSkip\">\n",
    "</h1>\n",
    "\n",
    "<h1 style='font-family: Optima;color:#ecac00'>\n",
    "04 Random Variables\n",
    "<a class=\"tocSkip\">   \n",
    "</h1>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532748f",
   "metadata": {},
   "source": [
    "## <span style='background:yellow; color:red'> Remember:<a class=\"tocSkip\"> </span>     \n",
    "\n",
    "+ Navigate to your `fmad2223` folder in the console/terminal.  \n",
    "+ Execute `git pull origin main` to update the code\n",
    "+ **Do not modify the files in that folder**, copy them elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98b151",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standard Data Science Libraries Import\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as scp\n",
    "import seaborn as sns\n",
    "\n",
    "#sns.set(rc={'figure.figsize':(12, 8.5)})\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa25e1",
   "metadata": {},
   "source": [
    "## Discrete Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc8ed9",
   "metadata": {},
   "source": [
    "### Theoretical Models Vs Empirical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec7d03",
   "metadata": {},
   "source": [
    "+ We begin with a simple mental experiment. Imagine we roll a dice (a honest not-loaded one) a million times and we look at the relative frequencies of every possible result. What is your guess for the numbers in the second row of this table?  \n",
    "$$\n",
    "\\quad\\\\\n",
    "\\begin{array}{|c|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{value} & 1 & 2 & 3 & 4 & 5 & 6 \\\\\n",
    "\\hline\n",
    "\\text{relative frequency} & ? & ? & ? & ? & ? & ? \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\quad\\\\\n",
    "$$\n",
    "    Those values that you clearly have in your mind are a **theoretical model** (your *prior*) of the outcome of this experiment. Of course, when we run the experiment and we get **empirical data** we do not expect the results to be a perfect match with the theory, because this is a **random experiment**. \n",
    "\n",
    "+ And that is precisely the notion of a discrete random variable $X$: *a theoretical model for the outcome of a random experiment with a finite number of possible outcomes.* More precisely (from the mathematical point of view) the result of the experiment is a discrete/countable set.\n",
    "\n",
    "+ Therefore, in order to describe a discrete random variable $X$ we need to provide its **probability density table or function**. That is a table of all the possible values of $X$ and their corresponding probabilities:\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\begin{array}{|c|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{value of }X: & x_1 & x_2 & \\cdots & x_k \\\\\n",
    "\\hline\n",
    "\\text{Probability for that value: }P(X = x_i) & p_1 & p_2 & \\cdots & p_k \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\quad\\\\\n",
    "$$\n",
    "where $p_1 + p_2 + \\cdots + p_k = 1$. Sometimes we will use *function notation* $f(x_i) = P(X = x_i)$, specially when we want to give a *formula* for the probability. We will soon see examples. \n",
    "\n",
    "+ The probability density function is sometimes called *probability mass function*. Thus you may see it abreviated to pdf or pmf, depending on the authors. For example, SciPy uses `pmf`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66215849",
   "metadata": {},
   "source": [
    "+ **Exercise:** use NumPy to do the experiment with a million dice rolls and get their absolute and relative frequency table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e44263",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load \"./exclude/S04-001.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f14ccd",
   "metadata": {},
   "source": [
    "### Mean and Variance for a Discrete Random Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629d400",
   "metadata": {},
   "source": [
    "+ A discrete random variable $X$ is therefore a theoretical model for the distribution of values of a random variable in a population. The **population mean** or **expectation** of $X$ represents the mean or average of the values that $X$ takes *in the population*. It is denoted with the greek letter $\\mu$ and also wit the symbol $E(X)$. When we need to clarify the random variable involved we will sometimes use a symbol such as $\\mu_X$.\n",
    "\n",
    "+ Similarly we define the **population variance** $\\sigma^2$, using all the values in the population. Both $\\mu$ and $\\sigma^2$ should be considered as abstract or also *hidden* values that we want to *estimate*, getting approximate values, but we can not actually obtain with certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4134991",
   "metadata": {},
   "source": [
    "+ One of the main goals of Statistics is to use sample data to estimate parameters of a population. Suppose that the discrete variable $X$ takes $n$ different values $x_1, x_2,\\ldots, x_k$. If we have a sample of $X$ and the absolute frequencies **in that sample** are $f_1, f_2, \\ldots,f_k$, then we can use that sample to give an estimate of the *population mean* $\\mu$ using the *sample mean*: \n",
    "$$\n",
    "\\quad\\\\\n",
    "\\bar X = \\dfrac{x_1 f_1 + \\cdots + x_k f_k}{n} = x_1 fr_1 + \\cdots + x_k fr_k\n",
    "\\quad\\\\\n",
    "$$  \n",
    "  where $fr_1, \\ldots, fr_k$ are the *sample relative frequencies*. It is very important that you realize that $\\bar X$ is an empirical quantity that comes out of a sample. Therefore it is something that we can compute using that sample that we have. On the other hand $\\mu$ is a theoretical quantity because it belongs to the population and we do not have access to the population (we would not be needing Statistic if we did!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc678f7",
   "metadata": {},
   "source": [
    "+ Now, looking at the last formula, recall that the relative frequencies are closely related with probabilities. In fact, the idea of probability first appeared as a theoretical model of the relative frequency. And so if we want to give a theoretical definition of the mean or expectation of a discrete random variable the only sensible choice is this:\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5'>\n",
    "$$\n",
    "\\fbox{\n",
    "$\n",
    "\\quad\\\\\n",
    "\\quad\\quad\\textbf{Mean of a Discrete Random Variable $X$ }\n",
    "\\quad\\\\\n",
    "\\mu = E(X) = x_1 p_1 + \\cdots + x_k p_k = \\sum_{i = 1}^k \\,x_i p_i\n",
    "$\n",
    "}\n",
    "$$\n",
    "</p> \n",
    "\n",
    "  That is, we have simply replaced relative frequencies with probabilities to go from empirical to theoretical. A similar reasoning lead to this expression for the:\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5'>\n",
    "$$\n",
    "\\fbox{\n",
    "$\n",
    "\\quad\\\\\n",
    "\\quad\\quad\\quad\\quad\\textbf{Variance of a Discrete Random Variable $X$}\n",
    "\\quad\\\\\n",
    "\\sigma^2 = \\operatorname{Var}(X) = \n",
    "(x_1 - \\mu)^2 p_1 + \\cdots + (x_k - \\mu)^2 p_k = \\sum_{i = 1}^k \\,(x_i -\\mu)^2 p_i\n",
    "$\n",
    "}\n",
    "$$\n",
    "</p> \n",
    "\n",
    "  The positive square root $\\sigma$ of the variance is called the **standard deviation** of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8416b4",
   "metadata": {},
   "source": [
    "**Exercise:** use Python (with NumPy or pandas) to compute $\\mu$ and $\\sigma^2$ for the random variable $X$ representing the outcome of a honest dice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ea600",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load \"./exclude/S04-002.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298a3ef",
   "metadata": {},
   "source": [
    "### Sampling Discrete Random Variables with Python\n",
    "\n",
    "\n",
    "+ Suppose we have a discrete random variable $X$ with values $x_1, \\ldots, x_k$ and corresponding probabilities $p_1, \\ldots, p_k$. In order to run simulations of our experiments with $X$ we would like to be able to use Python to generate synthetic random samples of $X$, according to its probability distribution. We can do that with this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0f74f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "values_X = np.arange(1, 7)\n",
    "probs_X = np.ones(shape = 6) / 6\n",
    "\n",
    "np.random.choice(values_X, 10, p=probs_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d700992b",
   "metadata": {},
   "source": [
    "### Operations on Discrete Random Variables\n",
    "\n",
    "+ **Example:** Assume that the population of interest is the set of households in a given city. And let the random variable $X$ represent the annual home insurance paid by each household. Similarly, let $Y$ represent the annual life insurance for each household. When we want to obtain the total amount of both insurance payments combined we need to consider the sum of the random variables $X + Y$. In many examples like this we would like to use the information about $X$ and $Y$ to obtain the properties of their sum $X + Y$ without having to redo the calculation. \n",
    "\n",
    "+ More generally, we are often interested in *linear combinations* of random variables, such as \n",
    "$$\n",
    "\\quad\\\\\n",
    "a\\,X + b\\,Y,\\qquad\\text{ where }a\\text{ and }b\\text{ are numeric coefficients.}\n",
    "\\quad\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b5570",
   "metadata": {},
   "source": [
    "+ The mean or expectation of such a linear combination is simply the same linear combination of the expectations  of the individual variables:\n",
    "$$\n",
    "\\quad\\\\\n",
    "E(a\\,X + b\\,Y) = a\\,E(X) + b\\,E(Y)\n",
    "\\quad\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa93bd",
   "metadata": {},
   "source": [
    "+ For the variance things get a little more complicated, because we need the notion of independence. Informally, $X$ and $Y$ are independent if knowledge about the value of $X$ does not affect the probability of the values of $Y$. The **covariance** of $X$ and $Y$ is\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\operatorname{cov}(X, Y) = E((X - \\mu_X)(Y - \\mu_Y))\n",
    "\\quad\\\\\n",
    "$$\n",
    "and the most general result says that\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\sigma^2(a\\,X + b\\, Y) = a^2\\,\\sigma^2_X + b^2\\,\\sigma^2_Y + 2\\,a\\,b\\, \\operatorname{cov}(X, Y)\n",
    "\\quad\\\\\n",
    "$$\n",
    "+ When $X$ and $Y$ are independent it turns out that $\\operatorname{cov}(X, Y) = 0$ (creful, it does not work the other way round) and therefore **in the independence case** we get a simpler formula:\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\sigma^2(a\\,X + b\\, Y) = a^2\\,\\sigma^2_X + b^2\\,\\sigma^2_Y\n",
    "\\quad\\\\\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7b005",
   "metadata": {},
   "source": [
    "### The Distribution Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc7fc4",
   "metadata": {},
   "source": [
    "+ The **(cumulative) distribution function (cdf)** $F_X$ of a random variable $X$ (discrete or continuous) is defined by:\n",
    "$$\n",
    "\\quad\\\\\n",
    "F_X(k) = P(X\\leq k)\\qquad\\text{ for any number }k\n",
    "\\quad\\\\\n",
    "$$\n",
    "You may think of $F(k)$ as the theoretical version of the table of cumulative relative frequencies. Therefore, it answers the question \"*what is the probability that $X$ takes a value $\\leq k$?*\"\n",
    "\n",
    "+ Because they are probabilities and because of their cumulative nature the typical graph for the distribution function of a discrete variable is a **stair shaped** graph like this one, climbing from 0 to 1 with a jump at each value of $X$ equal to the probability of that value:\n",
    "![](fig/04-01-FuncionDistribucionVariableAleatoria.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78cb7f8",
   "metadata": {},
   "source": [
    "## Binomial Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505620ec",
   "metadata": {},
   "source": [
    "### Bernouilli Random Variables\n",
    "\n",
    "+ A Bernouilli random variable is a very simple discrete random variable that only takes two values, 0 and 1, with the following probability table:\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\begin{array}{|l|c|c|}\n",
    "    \\hline\n",
    "    \\rule{0cm}{0.5cm}\\text{Value of }X:&1&0\\\\\n",
    "    \\hline\n",
    "    \\rule{0cm}{0.5cm}\\text{Probability for that value:}& p & q = 1 - p\\\\\n",
    "    \\hline\n",
    "\\end{array}\n",
    "\\quad\\\\\n",
    "$$\n",
    "These values 1 and 0 are (arbitrarily) called *success* and *failure* respectively.\n",
    "\n",
    "+ These Bernouilli type variables are useful because they are the building blocks for more complex types of variables, as we will soon see.\n",
    "\n",
    "+ **Example:** the variable $X = $ \"number of appearances of six when rolling a single dice\" is a Bernouilli variable with $p = 1/6$ and $q = 5/6$. We denote this with $X\\sim Bernouilli(p)$ (the symbol $\\sim$ is read \"is of type ...\")\n",
    "\n",
    "+ The mean of a random variable $X\\sim Bernouilli(p)$ is $\\mu = p$, and its variance is $\\sigma^2 = p\\cdot q = p(1 - p)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c27a33",
   "metadata": {},
   "source": [
    "### Binomial Random Variables\n",
    "\n",
    "+ **Example:** Suppose that we roll a dice 11 times and we use that experiment (the whole set of 11 rolls of the dice) to define a random variable $X$ where:\n",
    "$$\n",
    "\\quad\\\\\n",
    "X = \\textit{number of appearances of 6 in those 11 rolls of the dice}\n",
    "\\quad\\\\\n",
    "$$\n",
    "\n",
    "+ The situation in this example has these characteristics:\n",
    "\n",
    "  $(1)$ There is a **basic experiment**, rolling a dice in this case, that gets **repeated $n$ times** (in the example $n = 11$).  \n",
    "  \n",
    "  $(2)$  The $n$ repeated basic experiments are **independent** of each other. That is, the outcome of one of the experiments is not affected in any way by the outcome of the others.\n",
    "  \n",
    "  $(3)$  Every individual instance or trial of the basic experiment can only result in **success** (in the example, rolling a 6) represented with value $1$; o in **failure** (not rolling a 6) represented with value 0.  \n",
    "  \n",
    "  $(4)$  The **probability of success** for every trial is $p$ and that for failure is therefore $q = 1- p$. In the example $p = 1/6, q= 5/6$.  \n",
    "  \n",
    "  $(5)$ Finally, **our variable $X$ is the number of successful trials (with outcome 1) in the whole set of $n$ independent trials**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10388560",
   "metadata": {},
   "source": [
    "+ **Definition of Binomial Variable**  \n",
    "  A discrete random variable  $X$ with the above characteristics is a binomial variable with parameters $n$ and $p$, and we will use the symbol $X \\sim B(n, p)$ to denote this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3acaba",
   "metadata": {},
   "source": [
    "### Experiments with Binomial Variables using Python\n",
    "\n",
    "+ Let us see an example of a binomial variable. We will use the `prevalentHyp` variable in the `framingham` table that we have used in previous sessions. The variable takes the value 1 if the patient is hypertensive and 0 otherwise. Keep in mind that 1 and 0 are arbitrary, and so in this example *success* actually means that the patient is in fact hypertensive. \n",
    "\n",
    "+ **Exercise:**  \n",
    "    (a) Load the data table into the `framingham` pandas DataFrame. You have done this before.  \n",
    "    (b) Find the probability that a randomly chosen patient is hypertensive, and call it $p$.  \n",
    "    (c) Instead of choosing a single patient, suppose that we choose seven patients at random and with replacement. Let $X$ denote the number of hypertensive patients among those seven. What values can this variable $X$ actually take?  \n",
    "    (d) Use Python to choose a sample of seven patients (with replacement) and count the number of hypertensive patients in that sample.  \n",
    "    (e) Iterate the previous step $N = 50000$ times and store the 50000 results in a NumPy array called `X_samples`. Get a relative frequency table of the different values in `X_samples.`  \n",
    "(f) Choose the right plot to illustrate the contents of `X_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S04-003.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554995ff",
   "metadata": {},
   "source": [
    "### Probability Density for Binomial Variables\n",
    "\n",
    "+ The table of relative frequencies that you obtained in the previous exercise is an empirical approximation of the following expression for the following:\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5'>\n",
    "$$\n",
    "\\fbox{\n",
    "$\n",
    "\\quad\\\\\n",
    "\\textbf{Theoretical probability density of a binomial variable }X\\sim B(n, p)\n",
    "\\quad\\\\\n",
    "\\quad\\quad\\quad\\quad\n",
    "P(X = k) =\\displaystyle\\binom{n}{k}\\,p^k\\,q^{(n -k)}\\quad\\text{ for }\\quad k = 0, 1, 2, \\ldots, n\n",
    "$\n",
    "}\n",
    "$$\n",
    "</p>    \n",
    "where we recall $q = 1 - p$. Also the definition of the *binomial coefficient* is:\n",
    "$$\n",
    "\\dbinom{n}{k} = \\frac{\\overbrace{n\\left( n-1\\right) \\left( n-2\\right) \\cdots \\left( n-k+1\\right) }^{k\\mbox{ factors}}}{k!}\n",
    "$$\n",
    "where $k! = k\\cdot(k - 1)\\cdot(k - 2)\\cdot\\,\\cdots\\,\\cdot 2\\cdot 1$ is the factorial of $k$.\n",
    "\n",
    "+ Luckily you will not have to compute these by hand, Python will do the hard work for us. And it also gives us the chance to see how *painful* these binomial coefficients turn out to be. \n",
    "\n",
    "+ Let us use SciPy to obtain $\\dbinom{1000}{20}$. Note that these are not unreasonably large numbers, and yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c82ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "N = 100\n",
    "k = 20\n",
    "special.comb(N, k, exact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fca36e",
   "metadata": {},
   "source": [
    "+ This kind of result illustrates the fact that we should give up any hope of using the binomial distribution *by hand*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3667c28",
   "metadata": {},
   "source": [
    "### Mean and Variance of a Binomial Random Variable\n",
    "\n",
    "+ If $X \\sim B(n, p)$ then:\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\fbox{\n",
    "$\\begin{cases}\n",
    "\\mu = n\\cdot p\\\\[3mm]\n",
    "\\sigma^2 = n\\cdot p\\cdot q\n",
    "\\end{cases}$}\n",
    "\\quad\\\\\n",
    "$$\n",
    "Both results are very easy consequences of the fact that a $B(n, p)$ binomial is the sum of $n$ independent Bernouilli trials with the same $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed77278",
   "metadata": {},
   "source": [
    "### The binomial in Python\n",
    "\n",
    "+ For further details please check the [SciPy documentation website](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html#scipy.stats.binom).\n",
    "\n",
    "+ To compute concrete probability values of a binomial variable we use it *probability mass function (pmf)*, which is a fancy name for the table of probabilities.\n",
    "\n",
    "+ **Example:** Let $X\\sim B(7, 1/4)$ and let us compute the probability $P(X = 3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86932654",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "n = 7\n",
    "p = 0.25\n",
    "k = 3\n",
    "stats.binom.pmf(k, n, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad25823",
   "metadata": {},
   "source": [
    "+ We can also *freeze* the binomial variable and use the frozen version to compute this value but also the mean or variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b6502",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = stats.binom(n, p)\n",
    "\n",
    "print(\"Probability for k = \", k, \"is\", X.pmf(k))\n",
    "\n",
    "X_mean = X.mean()\n",
    "X_var = X.var()\n",
    "\n",
    "print(\"X_mean =\", X_mean)\n",
    "print(\"X_var =\", X_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daba803",
   "metadata": {},
   "source": [
    "+ To plot the probability mass of the distribution we often use a variant of the bar plot called a *stem and leaf* plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826da841",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "# First we create arrays with all posible values for this variable...\n",
    "x = np.arange(start = 0, stop = n + 1, step = 1)\n",
    "# ... and their probabilities:\n",
    "probs_x = stats.binom.pmf(x, n, p)\n",
    "# The next comand plots the \"leaf\" part\n",
    "ax.vlines(x, ymin = 0, ymax = probs_x, colors='b', lw=5, alpha=0.5)\n",
    "# And this one plots the \"stem\"\n",
    "ax.plot(x, probs_x, linestyle = '', marker = 'o', color ='blue', markersize=10)\n",
    "getPlot = ax.set_title(\"Binomial probability mass function (pmf)\", fontsize=20, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3362f2c",
   "metadata": {},
   "source": [
    "+ The cumulative distribution function (cdf) is similarly plotted with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955cfa08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "x = np.arange(start = 0, stop = n + 1, step = 1)\n",
    "# We replace probabilities with cumulative probabilities\n",
    "cumul_probs_x = stats.binom.cdf(x, n, p)\n",
    "ax.vlines(x, ymin = 0, ymax = cumul_probs_x, colors='b', lw=5, alpha=0.5)\n",
    "ax.plot(x, cumul_probs_x, linestyle = '', marker = 'o', color ='blue', markersize=10)\n",
    "getPlot = ax.set_title(\"Cumulative distribution function (cdf)\", fontsize=20, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854789f",
   "metadata": {},
   "source": [
    "+ **Random values from a binomial variable:** To generate random values of a binomial variable we will use either one of this methods:\n",
    "\n",
    "    + The NumPy classical method with `np.random.binomial(n, p, size)` (set the random seed with `np.random.seed(2022)`)\n",
    "    + The NumPy modern version with something like `np.random.default_rng(2022).binomial(n, p, size = 10)`.\n",
    "    + The Scipy version with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17835fba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Recall parameters\n",
    "print(\"Binomial B(n, p) with n =\", n, \", p = \", p)\n",
    "# NumPy legacy code\n",
    "np.random.seed(2022)\n",
    "print(\"Numpy legacy \\n\") \n",
    "print(np.random.binomial(n, p, size = 30))\n",
    "print(\"--\" * 12, \"\\n\")\n",
    "# NumPy current version\n",
    "print(\"Numpy current version \\n\") \n",
    "print(np.random.default_rng(2022).binomial(n, p, size = 30))\n",
    "print(\"--\" * 12, \"\\n\")\n",
    "# Scipy\n",
    "print(\"SciPy \\n\") \n",
    "print(stats.binom(n, p).rvs(size = 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21127f",
   "metadata": {},
   "source": [
    "+ **Note about reproducibility:** If you run the previous cell multiple times you will notice that the SciPy results do not change. That is because SciPy is also affected by the NumPy random seed. However if you remove the seed from the `default_rng` you will notice that ironically the current version of NumPy is not affected by the legacy NumPy random seed generator. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203604e",
   "metadata": {},
   "source": [
    "+ **Exercise:**    \n",
    "    (a) use Python to get the theoretical probabilities for the binomial variable of the preceding exercise. Recall that $X$ = number of hypertensive patients in a sample (with replacement) of seven patients from the framingham data set.  \n",
    "    (b) Compare the theoretical values with the relative frequencies that you obtained in that exercise for $N = 50000$ samples of size 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729cbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S04-004.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cd39f",
   "metadata": {},
   "source": [
    "### The Binomial Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee2bf8",
   "metadata": {},
   "source": [
    "+ There are infinitely many types of binomials, depending on the combination of values for $n$ and $p$. But we can single out three particular cases, each of them requiring a different approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a76ddb",
   "metadata": {},
   "source": [
    "#### Binomials with small $n$\n",
    "\n",
    "+ For example, the $X\\sim B(7, 1/4)$ that we have been using in the previous examples. Let us plot its probability distribution again, but this time we add a bar plot to the stem and leaf plot, to ease the comparison between the global shape of this binomial and the ones we will see below. The best way to work with a binomial like this is to use directly the pmf and cdf functions to get the probability values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12991c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "n = 7\n",
    "p = 0.25\n",
    "# First we create arrays with all posible values for this variable...\n",
    "x = np.arange(start = 0, stop = n + 1, step = 1)\n",
    "# ... and their probabilities:\n",
    "probs_x = stats.binom.pmf(x, n, p)\n",
    "# The next comand plots the \"leaf\" part\n",
    "ax.vlines(x, ymin = 0, ymax = probs_x, colors='b', lw=5, alpha=0.3)\n",
    "ax.bar(x, height = probs_x, color='tan', width = 1, lw=1.5, edgecolor='black',linestyle=\"-\")\n",
    "# And this one plots the \"stem\"\n",
    "getPlot = ax.plot(x, probs_x, linestyle = '', marker = 'o', color ='blue', markersize=5, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433a8f6",
   "metadata": {},
   "source": [
    "#### Binomials with large $n$ but $p$ very small (close to $0$) or big (close to $1$).\n",
    "\n",
    "+ In this case instead of using the binomial it is best to use the approximation provided by the **Poisson distribution**. We will talk about it later. Here we just plot an example with $n = 200$ but $p = 0.001$. *Please note that to improve the visualization we have reduced the range of the x axis to $[0, 50]$ instead of the the actual set of values of $X$ which is $[0, 200]$*. But the probabilities for larger values of $X$ are negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddea18e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "n = 200\n",
    "p = 0.01\n",
    "x = np.arange(start = 0, stop = n + 1, step = 1)\n",
    "probs_x = stats.binom.pmf(x, n, p)\n",
    "getPlot = ax.bar(x, height = probs_x, color='tan', width = 1, lw=1.5, edgecolor='black',linestyle=\"-\")\n",
    "plt.xlim(left=0, right=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f8c0b",
   "metadata": {},
   "source": [
    "#### Binomials with large $n$ but moderate $p$\n",
    "\n",
    "+ By this we mean that $n$ is hundreds or more, while $p$ is an intermediate probability, not too close to 0 or 1. Let us plot the example where $n= 100$ while $p = 1/3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2304cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "n = 100\n",
    "p = 1/3\n",
    "x = np.arange(start = 0, stop = n + 1, step = 1)\n",
    "probs_x = stats.binom.pmf(x, n, p)\n",
    "ax.bar(x, height = probs_x, color='tan', width = 1, lw=1.5, edgecolor='black',linestyle=\"-\")\n",
    "getPlot = plt.xlim(left=0, right=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18538be2",
   "metadata": {},
   "source": [
    "+ The bar plot is starting to look like a bell shaped curve. That, as you have probably already guessed, is not a coincidence. **Abraham De Moivre** showed that as $n$ gets bigger (keeping $p$ moderate) this plot resembles more and more a normal curve, with  \n",
    "$$\\mu = n\\cdot p, \\sigma = \\sqrt{n\\cdot p\\cdot q}$$\n",
    "as was to be expected.\n",
    "+ This was a very important result at that time, because working directly with these kind of binomials proved to be impossible. For example, if $X\\sim B(100, 1/3)$ to compute the probability of the interval \n",
    "$$\n",
    "\\quad\\\\\n",
    "P(25 \\leq X \\leq 35) = P(X = 25) + P(X = 26) + \\cdots + P(X = 34) + P(X= 35)\n",
    "\\quad\\\\\n",
    "$$\n",
    "you need to compute quite a few terms like this one:\n",
    "$$\n",
    "\\quad\\\\\n",
    "P(X = 29) = \\dbinom{100}{29}\\left(\\dfrac{1}{3}\\right)^{29}\\left(\\dfrac{2}{3}\\right)^{71}\n",
    "\\quad\\\\\n",
    "$$\n",
    "and in particular you will have to find out that\n",
    "$$\n",
    "\\binom{100}{29} = \\dfrac{100!}{29!\\,\\, 71!} = \n",
    "\\dfrac{100\\cdot 99 \\cdot 98\\cdots 73 \\cdot 72}{29\\cdot 28 \\cdot 27\\cdots  2\\cdot 1} = 1917353200780443050763600\n",
    "$$\n",
    "+ Doing computations like that, particularly when they had to be done by hand, rendered the binomial useless. But the probability we are trying to compute is simply the area of the blue colored region in this plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b75886",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "n = 100\n",
    "p = 1/3\n",
    "x = np.arange(start = 0, stop = n + 1, step = 1)\n",
    "probs_x = stats.binom.pmf(x, n, p)\n",
    "ax.bar(x, height = probs_x, color='tan', width = 1, lw=1.5, edgecolor='black',linestyle=\"-\")\n",
    "x1 = np.arange(start = 25, stop = 35 + 1, step = 1)\n",
    "probs_x1 = stats.binom.pmf(x1, n, p)\n",
    "ax.bar(x1, height = probs_x1, color='blue', width = 1, lw=1.5, edgecolor='black',linestyle=\"-\")\n",
    "getPlot = plt.xlim(left=0, right=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ad628",
   "metadata": {},
   "source": [
    "+ But in problems like this, and as $n$ gets bigger, the difference between the individual bars in this plot becomes less and less relevant. That is way we are more and **more interested in intervals than we are in individual values**. The probability of an individual value is tending to zero as $n$ increases. This is another instance of the transition from discrete to continuous. And when we look at this plot from that perspective, the answer is that we are trying to obtain the blue area under the normal curve. We can do that using **integral calculus**, which had just been discovered when de Moivre worked on this problem. The figure below illustrates this for a different binomial variable:\n",
    "\n",
    "![](./fig/04-04-BinomialVsNormal.png)\n",
    "\n",
    "+ Using integrals may seem at first like a complicated way to work. But if you compare it with the computation of binomial coefficients, it turns out to be a very useful simplification. The most important consequence of this, regardless of the technical details, is that this example led people to discover that they could define probability as the area under a curve. And that is the idea behind the definition of a continuous random variable. And this way of assigning probability takes us away from the Laplace Method and into a more abstract and mature concept of Probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbba6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from IPython.display import IFrame\n",
    "#IFrame(\"https://www.geogebra.org/m/q4buqzfp\",800,800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1006437",
   "metadata": {},
   "source": [
    "## Continuous Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df13df0f",
   "metadata": {},
   "source": [
    "+ Let us look into that idea of using the area under the graph of a function to assign probabilities. Not every function will do for this, but the requirements are quite simple.\n",
    "\n",
    "+ A **(continuous) density function** has two properties:\n",
    "    + It is **non-negative:** $f(x)\\geq 0$ for all $x$.\n",
    "    + The **total area under $f(x)$ is 1**. That is:\n",
    "    <p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 50%;'>\n",
    "    $$\n",
    "    \\quad\\\\\n",
    "    \\int_{-\\infty}^{\\infty}f(x)\\,dx=1\n",
    "    \\quad\\\\\n",
    "    $$\n",
    "    </p>\n",
    "    In particular $f$ must be integrable, but we will not look into the technical meaning of that condition.\n",
    "\n",
    "+ Given a continuous density function we can use it to define a **continuous random variable $X$**, by saying that the probability of $X$ taking value in an interval $[a, b]$ is:\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 60%;'>\n",
    "$$\n",
    "\\quad\n",
    "P(a \\leq X \\leq b) = \\text{area under the graph of }f = \\int_a^bf(x)\\,dx.\n",
    "\\quad\n",
    "$$\n",
    "</p>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080950e",
   "metadata": {},
   "source": [
    "### Understanding the role of the density function\n",
    "\n",
    "+ The figure below shows a typical continuous density function and the way to interpret its values.\n",
    "![](./fig/04-05-InterpretacionFuncionDensidadFicticia.png)\n",
    "That is why we say that a density function defines a distribution (a way to allocate) probability. In the case of a discrete random variable the table of values vs probabilities does the same job. But now we have a function $f$ to assign probabilities. The function is a *continuous table* of sorts. And if the probability table of a discrete random variable is the theoretical version of the table of relative frequencies, you may now think of this continuous density function as the theoretical version of the density curves that we plotted when exploring the data in a sample.\n",
    "\n",
    "+ It is very important to keep in mind that **the height of the density function is not the probability. Probability is given by the area under the curve.** They are connected, but different entities.\n",
    "\n",
    "+ In particular, and this may seem paradoxical at first, the probability value of a single point in a continuous random variable is always zero:\n",
    "$$\n",
    "\\quad\\\\\n",
    "P(X = x_0) = 0,\\,\\text{ for any value }\\,x_0\n",
    "\\quad\\\\\n",
    "$$\n",
    "It is the probability of intervals (not points) what matters here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00cdfd",
   "metadata": {},
   "source": [
    "### Mean and Variance for a Continuous Random Variable\n",
    "\n",
    "+ Recall that for a discrete random variable with values $x_1,\\ldots, x_k$ and probabilities $p_1, \\ldots, p_k$ we had:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\mu = E(X) = \\displaystyle\\sum_{i = 1}^k x_i\\cdot p_i\\\\[3mm]\n",
    "\\sigma^2 = Var(X) = \\displaystyle\\sum_{i = 1}^k (x_i - \\mu)^2\\cdot p_i\n",
    "\\end{array}\n",
    "$$\n",
    "In the case of a continuous random variable $X$ with density function $f(x)$ we have\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 60%;'>\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\begin{array}{l}\n",
    "\\mu = E(X) = \\displaystyle\\int_{-\\infty}^{\\infty} x\\cdot f(x)\\, dx\\\\[5mm]\n",
    "\\sigma^2 = Var(X) = \\displaystyle\\int_{-\\infty}^{\\infty} (x - \\mu)^2\\cdot f(x)\\, dx\n",
    "\\end{array}\n",
    "\\quad\\\\\n",
    "$$\n",
    "</p>\n",
    "\n",
    "Note that going from discrete to continuous is achieved by changing the sum for an integral and the probability $p_i$ for the *probability differential* $dp = f(x)\\, dx$, which represents the probability of and *infinitesimally small interval*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f75b11f",
   "metadata": {},
   "source": [
    "### The (Cumulative) Distribution Function of a Continuous Random Variable\n",
    "\n",
    "+ The distribution function for a random variable (discrete or continuous) is:\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 30%;'>\n",
    "$$\n",
    "\\quad\\\\\n",
    "F(k) = P(X \\leq k)\n",
    "\\quad\\\\\n",
    "$$\n",
    "</p>\n",
    "and for a continuous random variable that turns out to be\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 30%;'>\n",
    "$$\n",
    "\\quad\\\\\n",
    "F(k) = \\int_{-\\infty}^{k}f(x)\\,dx\n",
    "\\quad\\\\\n",
    "$$\n",
    "</p>\n",
    "\n",
    "+ For discrete random variables the distribution function looked like a stair. In the continuous case it is more like a ramp, climbing from 0 to 1 as $x$ moves left to right along the horizontal axis:\n",
    "![](./fig/04-06-FuncionDistribucionVariableContinuaTipica.png)\n",
    "Note that (unlike the density function) the values of the function are indeed probabilities (cumulative ones, of course). That is one of the reasons that make distribution functions useful:\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 30%'>\n",
    "$$\n",
    "\\quad\\\\\n",
    "P(a < X < b) = F(b) - F(a)\n",
    "\\quad\\\\\n",
    "$$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77373b",
   "metadata": {},
   "source": [
    "## The Uniform Distribution\n",
    "\n",
    "+ This is possibly the simplest of all continuous random variables, yet it is extremely important and useful. It represents the idea of randomly picking a point in interval $[a, b]$ in such a way that all regions of the interval with the same size are equally probable.  \n",
    "(*Side Note:* sometimes people say \"all points are equally likely\" to describe the uniform distribution. But that is misleading because the probability of a single point is always zero in a continuous random variable, be it *uniform or not*).\n",
    "\n",
    "+ The density function of a uniform variable $X$ is therefore constant in the interval $[a, b]$ and zero elsewhere:\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 60%;'>\n",
    "$$\n",
    "\\quad\\\\\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "\\dfrac{1}{b - a} & \\text{ if } a\\leq x\\leq b\\\\[3mm]\n",
    "0 & \\text{ otherwise }\\\\\n",
    "\\end{cases}\n",
    "\\quad\\\\\n",
    "$$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec67b40",
   "metadata": {},
   "source": [
    "### Mean and Variance for Uniform Distributions\n",
    "\n",
    "+ The mean of a uniform variable is, as expected:\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 30%;'>\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\mu = \\dfrac{a + b}{2}\n",
    "\\quad\\\\\n",
    "$$\n",
    "</p>    \n",
    "and the variance is\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 30%;'>\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\sigma^2 = \\dfrac{(b - a)^2}{12}\n",
    "\\quad\\\\\n",
    "$$\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae15a5cc",
   "metadata": {},
   "source": [
    "### Uniform Distributions with Python\n",
    "\n",
    "+ The most important operation with random uniform variables in Python is the ability to generate random values. We can do that with NumPy or SciPy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us generate an array with N random points in [a, b]\n",
    "N = 20000\n",
    "a = -5\n",
    "b = 12\n",
    "\n",
    "# With current NumPy version \n",
    "# See https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.uniform.html\n",
    "\n",
    "print(\"NumPy current version:\")\n",
    "X = np.random.default_rng(2022).uniform(low = a, high = b, size = N)\n",
    "print(X[0:20])\n",
    "\n",
    "print(\"-\" * 80, \"\\n\")\n",
    "\n",
    "# With legacy NumPy versions\n",
    "# See https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html\n",
    "\n",
    "print(\"NumPy legacy:\")\n",
    "np.random.seed(2022)\n",
    "X =  np.random.uniform(low = a, high = b, size = N)\n",
    "print(X[0:20])\n",
    "print(\"-\" * 80, \"\\n\")\n",
    "\n",
    "# Scipy\n",
    "print(\"SciPy:\")\n",
    "X = stats.uniform(loc = a, scale = b - a).rvs(size = N)\n",
    "print(X[0:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8b087",
   "metadata": {},
   "source": [
    "+ Recall that the *size* parameter of NumPy random generators is in fact a *shape*. That is, you can easily modify the above code to get a matrix of uniform random numbers with any desired shape.\n",
    "\n",
    "+ **Exercise:**  \n",
    "(a) Using the above results check the formulas for the mean and variance of the uniform distribution.  \n",
    "(b) Generate a hundred million points uniformly distributed in the square $[-1, 1]\\times[-1, 1]$. Count how many of them are at a distance less \n",
    "than 1 from the origin.  \n",
    "(c) Plot the first $n = 5000$ points and those among them that verify the condition about the distance.  \n",
    "(d) Use your results to estimate the number $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b169af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S04-005.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58234b4",
   "metadata": {},
   "source": [
    "## Normal Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfadea4",
   "metadata": {},
   "source": [
    "### The Normal Curves\n",
    "\n",
    "+ We have already met these curves more than once. There is in fact a whole [family of normal curves](https://www.geogebra.org/m/BE4aF2Yb), depending on their mean and variance, whose equation is\n",
    "$$\n",
    "f_{\\mu,\\sigma}(x)=\\displaystyle\\dfrac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n",
    "$$\n",
    "But fear not, we will not be using that much!\n",
    "\n",
    "+ The choice of $\\mu$ and $\\sigma$ determines the shape of this bell curves. More specifically, $\\mu$ sets the *center* of the curve (which is always symmetrical), while $\\sigma$ controls if the bell is tall and narrow (for small $\\sigma$) or short and wide (for bigger $\\sigma$). \n",
    "![](./fig/04-07-CurvasNormalesZoo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4375e64",
   "metadata": {},
   "source": [
    "### Normal Random Variables\n",
    "\n",
    "+ A continuous random variable $X$ whose density curve is $f_{\\mu,\\sigma}(x)$ is a **normal random variable** and we write $X\\sim N(\\mu, \\sigma)$ to denote a random variable with mean $\\mu$ and standard deviation $\\sigma$.  \n",
    " **Note:** be careful, many authors use $N(\\mu, \\sigma^2)$ instead. Check always the convention being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866997cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from IPython.display import IFrame\n",
    "#IFrame(\"https://www.geogebra.org/m/egtwaagt\",800,800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7c3e6",
   "metadata": {},
   "source": [
    "### Direct Probability Problems for Normal Variables with Python: Tails and Intervals\n",
    "\n",
    "+ The first skill that we require is the ability to compute the probability of a given interval. That is, given $X\\sim N(\\mu, \\sigma)$ we want to obtain \n",
    "$$P(a < X < b)$$\n",
    "for any choice of $a$ and $b$. In particular we would also like to compute the probability of a **left tail**\n",
    "$$P(X < b)$$\n",
    "or a **right tail**\n",
    "$$P(a < X)$$\n",
    "\n",
    "+ **Example:** Given $X\\sim N(10, 2)$ let us compute the probability of the left tail \n",
    "$$P(X < 10.5)$$\n",
    "If you recall how we used Scipy with the binomial, this will looks familiar. For further details [check the SciPy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb7ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "mu = 10\n",
    "sigma = 2\n",
    "\n",
    "b = 10.5\n",
    "\n",
    "stats.norm.cdf(b, loc = mu, scale = sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf10c76",
   "metadata": {},
   "source": [
    "+ **Example:** If instead we want the probability of a right tail such as \n",
    "$$P(X > 11)$$\n",
    "(still with $X\\sim N(10, 2)$) then we can either use basic probability properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 11\n",
    "1 - stats.norm.cdf(a, loc = mu, scale = sigma) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f231a",
   "metadata": {},
   "source": [
    "or we can use the so called *survival function* `sf`, which is nothing but `1 - cdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c708f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.sf(a, loc = mu, scale = sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee998c4",
   "metadata": {},
   "source": [
    "![](./fig/04-09-EjemplosProbabilidadNormal01.png)\n",
    "\n",
    "+ **Example:** For the probability of an interval such as \n",
    "$$P(7<X<12)$$\n",
    "(same $\\mu$ and $\\sigma$ as before) we simply take the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83275854",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 7\n",
    "b = 12\n",
    "stats.norm.cdf(b, loc = mu, scale = sigma)  - stats.norm.cdf(a, loc = mu, scale = sigma) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b19af",
   "metadata": {},
   "source": [
    "![](./fig/04-09-EjemplosProbabilidadNormal03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4f99c",
   "metadata": {},
   "source": [
    "### Inverse Probability Problems for Normal Variables with Python\n",
    "\n",
    "+ The **inverse probability problem (left tail)** for a normal variable $X\\sim N(\\mu, \\sigma)$ is this: given a probability $p$ find the value $k$ such that:  \n",
    "$$P(X < k) = p$$\n",
    "\n",
    "+ **Example:** given a $N(10, 2)$ normal, find the value $k$ for which \n",
    "$$\n",
    "\\quad\\\\\n",
    "P(X\\leq k) = \\dfrac{1}{3}\n",
    "\\quad\\\\\n",
    "$$\n",
    "![](./fig/04-09-EjemplosProbabilidadNormal04.png)  \n",
    "In order to answer with Python we will use the `ppf` (percentile point function) function from SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 10\n",
    "sigma = 2\n",
    "\n",
    "p = 1/3\n",
    "\n",
    "stats.norm.ppf(p, loc = mu, scale = sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d565d",
   "metadata": {},
   "source": [
    "+ **Exercise:**  given a $N(0, 1)$ normal, find the value $k$ for which \n",
    "$$P(X\\leq k) = 0.975$$ \n",
    " **Note:** This exercise is very important for Statistical Inference. We will need the answer when we discuss confidence intervals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21abe48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S04-006.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567f327",
   "metadata": {},
   "source": [
    "From [XKCD](https://xkcd.com/2118/)\n",
    "![](https://imgs.xkcd.com/comics/normal_distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef702c69",
   "metadata": {},
   "source": [
    "### Generating Random Normal Values\n",
    "\n",
    "+ Another important skill when working with random variables is the ability to generate random values from $X\\sim N(\\mu, sigma)$. As was the case for the binomial, we have several different options, using NumPy or SciPy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7beaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal variable parameters\n",
    "\n",
    "mu = 10\n",
    "sigma = 2\n",
    "\n",
    "# Number of generated randm values\n",
    "\n",
    "N = 25\n",
    "\n",
    "# NumPy legacy code\n",
    "np.random.seed(2022)\n",
    "print(\"Numpy legacy \\n\") \n",
    "print(np.random.normal(loc = mu, scale = sigma, size = N))\n",
    "print(\"--\" * 12, \"\\n\")\n",
    "# NumPy current version\n",
    "print(\"Numpy current version \\n\") \n",
    "print(np.random.default_rng(2022).normal(loc = mu, scale = sigma, size = N))\n",
    "print(\"--\" * 12, \"\\n\")\n",
    "# Scipy\n",
    "print(\"SciPy \\n\") \n",
    "print(stats.norm(loc = mu, scale = sigma).rvs(size = N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b746f4",
   "metadata": {},
   "source": [
    "+ **Exercise:** recall that in a previous exercise we generated and plotted a million points uniformly distributed in the square $[-1, 1]\\times[-1, 1]$. Repeat that code again but this time generate only 10000 points. Next, generate 10000 more points $(x, y)$ where both $x$ and $y$ are normal random variables with $\\mu = 0, \\sigma = 1$. Add this second set of points to the same plot using a different color (hint: use `alpha` to control overplotting). Do you see the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35070dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S04-007.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87057e8",
   "metadata": {},
   "source": [
    "## The Standard Normal $Z$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad6daa",
   "metadata": {},
   "source": [
    "+ If $X\\sim N(\\mu, \\sigma)$ is any normal variable, then the **standardized** variable\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 30%;'>\n",
    "$$\n",
    "\\quad\\\\\n",
    "Z=\\dfrac{X-\\mu}{\\sigma}\n",
    "\\quad\\\\\n",
    "$$\n",
    "</p>\n",
    "\n",
    "  is always a normal variable of type $N(0, 1)$. This specially important normal variable with $\\mu = 0$ and $\\sigma = 1$ is called the  **standard normal** and is always denoted by $Z$ in Statistics. \n",
    "  \n",
    "+ Thus the process of standardization converts the values of any normal into values of $Z$. The standard normal $N(0, 1)$ can therefore be considered as a *universal scale* for all normal variables. If you need to understand how *typical* or *unexpected*  a value from a normal variable is, you just have to standardize it.\n",
    "\n",
    "+ The **68-95-99 Rule**. As a consequence of the above, whenever we work with normal variables we can be sure that these approximations always hold:\n",
    "<p style='font-family: Optima;color:blue;background-color:#f0f0f5;width: 30%;'>\n",
    "$$\n",
    "\\quad\\\\\n",
    "\\begin{cases}\n",
    "P(\\mu-\\sigma<X<\\mu+\\sigma)\\approx 0.683,\\\\[3mm]\n",
    "P(\\mu-2\\sigma<X<\\mu+2\\sigma)\\approx 0.955\\\\[3mm]\n",
    "P(\\mu-3\\sigma<X<\\mu+3\\sigma)\\approx 0.997\n",
    "\\end{cases}\n",
    "\\quad\\\\\n",
    "$$\n",
    "</p>\n",
    "  \n",
    "  irrespective of the values of $\\sigma$ and $\\mu$. Going back to that idea of *universal probability scale* that means that if a value of a random variable is four or more sigmas away from the mean, it can positively be catalogued as *weird*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c7b2e",
   "metadata": {},
   "source": [
    "+ **Exercise:**  \n",
    "$(a)$ check the 68-95-99 rule using Python e.g for $N(40, 3.6)$. Change $\\mu$ and $\\sigma$ a few times and recheck.  \n",
    "$(b)$ If $X \\sim N(123, 17)$ and we observe the value $168$, how *weird* is this value? Make your answer quantitative; that is, answer with a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0fa0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S04-008.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a949eb16",
   "metadata": {},
   "source": [
    "### Sums of Normal Variables\n",
    "\n",
    "+ If $X_1\\sim N(\\mu_1,\\sigma_1)$ and $X_2\\sim N(\\mu_2,\\sigma_2)$ are  **independent normal variables**, then their sum is **also a normal variable** of type:\n",
    "<p style='color:blue;background-color:#f0f0f5;width: 30%;'>\n",
    "$$\n",
    "N\\left(\\mu_1+\\mu_2,\\sqrt{\\rule{0cm}{0.5cm}\\sigma_1^2+\\sigma_2^2}\\right)\n",
    "$$\n",
    "</p>\n",
    "  \n",
    "  We emphasize that the important thing here is the fact that the sum is also normal. If you e.g. sum independent binomials you will not get a new binomial. This property generalizes to the sum of $k$ independent normals that again result in a normal of type:\n",
    "$${N}\\left(\\mu_1+\\cdots+\\mu_k,\\sqrt{\\rule{0cm}{0.5cm}\\sigma_1^2+\\cdots+\\sigma_k^2}\\right).$$  \n",
    "\n",
    "+ The **mixture** of random normals is an entirely different process, which often results in multimodal variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593badbd",
   "metadata": {},
   "source": [
    "+ **Exercise:**  \n",
    "$(a)$ Generate a pair of large samples (`N = 3000`) of the same size from $X_1\\sim N(-3, 1)$ and $X_2\\sim N(2, 1/2)$. Combine those samples to get a sample of the normal variable $X_3 = 3 X_1 + 4 X_2$.  \n",
    "$(b)$ What type of normal is $X_3$? That is, what are the theoretical values of $\\mu$ and $\\sigma$ for $X_3$?  \n",
    "$(c)$ Check these against your empirical values.  \n",
    "$(d)$ Plot the density curve of the $X_3$ sample. Read this page from the [SciPy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) and find out how to add to the plot the theoretical `pdf` of this normal random variable $X_3$. Does it confirm the above result?  \n",
    "$(e)$ Now let us consider the *mixture* of $X1$ and $X2$. For this you only need to concatenate both NumPy arrays. Look at this [Section on Array Concatenation from the Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html#Array-Concatenation-and-Splitting) to learn how to do this kind of operations on arrays of various shapes. Once you have concatenated the arrays plot (in a new, different plot) the density curve of the resulting sample. How does it look like? is it normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152bd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"./exclude/S04-009.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956d83b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
